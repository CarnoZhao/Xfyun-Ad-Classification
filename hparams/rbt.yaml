learning_rate: 2.0e-05
model_name: hfl/chinese-roberta-wwm-ext
num_epochs: 30
batch_size: 64
fold: -1
num_classes: 137
smoothing: 0.1
alpha: 0
max_length: 256
drop_rate: 0.3
name: text/rbt
version: sorted_all
